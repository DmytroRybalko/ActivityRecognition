---
title: "Modelling activity 7: standing while talking"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

library(tidyverse)
library(caTools)
library(ROCR)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(e1071)
```

# Agenda:

1) Type of datesets: raw, normalized, partial normalized  
2) PCA exploring, correlation finding, reduction dimension  
3) Cross-validation  
4) Classification algorithms:  
4.1) decision tree DS;  
4.2) random forest;  
4.3) k-NN;  

# 1.Data preparing

## Load features:

```{r}
raw_features7 <- readRDS("features_activity7_v1.rds")
#norm_features <- readRDS("data/preprocessing/norm_features_v1.rds")
```

Data looks like:
```{r}
raw_features7
```

Convert user_id value to factor:
```{r}
raw_features7 <- raw_features7 %>% 
  mutate_at("user_id", ~factor(., levels = as.character(1:10)))
#str(raw_features1)
```

Explore raw_features1 data

Find NA's:
```{r}
raw_features7 %>% 
  is.na() %>% 
  table()
```

Identify row with NA's
```{r}
which(rowSums(is.na(raw_features7)) == TRUE)
```

Find row with NA's
```{r}
raw_features7[which(rowSums(is.na(raw_features7)) == TRUE), ]
```

Replace NA's with median:
```{r}
raw_features7 <- raw_features7 %>% 
  mutate_at("peak_sd_dist_ax", ~if_else(is.na(.),
                                        median(raw_features7$peak_sd_dist_ax, na.rm = T), .)) 
```

Check NA's:
```{r}
raw_features7 %>% 
  is.na() %>% 
  table()
```

## 2 Split data into a training and test sets

```{r}
set.seed(307)
spl.a7 <- sample.split(raw_features7$user_id, SplitRatio = 0.7)
```
Now, let's create our training and testing sets using the subset function.
```{r}
train.data.a7 <- subset(raw_features7, spl.a7 == TRUE)
test.data.a7 <- subset(raw_features7, spl.a7 == FALSE)
```

## 3.Cross-validation   

First, we need to define how many folds we want. We can do this using the trainControl function.  
```{r}
numFolds <- trainControl(method = "cv", number = 10)
```
  
Then we need to pick the possible values for our cp parameter, using the expand.grid function. So we'll call it cpGrid, and then use expand.grid, where the only argument is .cp = seq(0.01,0.5,0.01). This will define our cp parameters to test as numbers from 0.01 to 0.5, in increments of 0.01.
```{r}
cpGrid <- expand.grid(.cp = seq(0.01, 0.5, 0.01))
```
  
Now, we're ready to perform cross validation. 
```{r, cache=TRUE, eval=FALSE}
train(user_id ~ ., data = train.data.a7, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
```

The final value used for the model was cp = 0.01. This is the cp value we want to use in our CART model.  
  
So now let's create a new CART model with this value of cp, instead of the minbucket parameter.

# 4.Classification algorithms

## 4.1. Decision Tree (DS)

### 4.1.1 Create the model with train data!

```{r}
base.CART.a7 <- rpart(user_id ~ ., data = train.data.a7, method = "class", cp = 0.01)
```
Now let's plot our tree using the prp function
```{r}
prp(base.CART.a7)
```

### 4.2.1 Make prediction on train data

Now let's see how well our CART model does at making predictions for the **train** set.
```{r}
predict.CART.train.a7 <- predict(base.CART.a7, newdata = train.data.a7, type = "class")
```
Now let's compute the accuracy of our model by building a confusion matrix.
```{r}
confusionMatrix(reference = train.data.a7$user_id, data = predict.CART.train.a7,         
                mode = "prec_recall", dnn = c("Reference", "Prediction"))
```

### 4.1.2 Make prediction on test data

Now let's see how well our CART model does at making predictions for the **test** set.
```{r}
predict.CART.test.a7 <- predict(base.CART.a7, newdata = test.data.a7, type = "class")
```
Now let's compute the accuracy of our model by building a confusion matrix.
```{r}
confusionMatrix(reference = test.data.a7$user_id, data = predict.CART.test.a7,         
                mode = "prec_recall", dnn = c("Reference", "Prediction"))
```

## 4.2 Randome Forest

Build the model
```{r}
random.forest.a7 <- randomForest(user_id ~ ., data = train.data.a7, nodesize = 25, ntree = 200, importance = T)
```

Make prediction
```{r}
predict.RF.test.a7 <- predict(random.forest.a7, newdata = test.data.a7)
```

See result
```{r}
confusionMatrix(reference = test.data.a7$user_id, data = predict.RF.test.a7,         
                mode = "prec_recall", dnn = c("Reference", "Prediction"))
```

Random Forest importance parameter:
```{r}
#nn <- rownames(random.forest$importance)
MeanDecreaseGini <- random.forest.a7$importance %>%
  as_tibble(rownames = "features") %>% 
  select(features, MeanDecreaseAccuracy, MeanDecreaseGini) %>% 
  arrange(desc(MeanDecreaseGini))
MeanDecreaseGini
```

```{r}
importance(random.forest.a1, type = 2)
```

# TODO Build bar chart!
